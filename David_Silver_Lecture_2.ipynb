{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "David Silver Lecture 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtMtZX0tdwYT",
        "colab_type": "text"
      },
      "source": [
        "# Lecture 2\n",
        "\n",
        "This Document contains the simulation of examples provided in Lecture 2 of Introduction to Rienforcement Learning by David Silver\n",
        "\n",
        "Author: Krishna Chaitanya Kosaraju\n",
        "\n",
        "email: kkrishnachaitanya89@gmail.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHbYA6yLqWr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yISUI0np_P3W",
        "colab_type": "text"
      },
      "source": [
        "#Part one \n",
        "\n",
        "State Value function for Markov Reward Process (MRP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV_SwboBrJ1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Propability Trasition Matrix\n",
        "# C1, C2, C3, Pass, Pub, FB, Sleep\n",
        "#              C1 , C2,  C3, Pass, Pub, FB,  Sleep\n",
        "P = np.array([[0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0],\n",
        "              [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.2],\n",
        "              [0.0, 0.0, 0.0, 0.6, 0.4, 0.0, 0.0],\n",
        "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
        "              [0.2, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0],\n",
        "              [0.1, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0],\n",
        "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n",
        "\n",
        "# test print('Column sum of the Propability transition matrix is', np.dot(P,np.full(7,1)))\n",
        "# MDP states and their Rewards\n",
        "#r = np.array([-2, -2, -2, 10, 1, -1, 0])\n",
        "r = {'C1' : -2,\n",
        "     'C2' : -2,\n",
        "     'C3' : -2,\n",
        "    'Pass': 10,\n",
        "     'Pub': 1,\n",
        "     'FB' :-1,\n",
        "     'Sleep': 0}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRvPdWrwrSfQ",
        "colab_type": "code",
        "outputId": "fd87a911-a2cb-42a0-ab3c-613c90ac3929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "def net_reward(walk, gamma = 0.5):\n",
        "  \"\"\" Return the net discounted reward for the given list of steps in 'walk',\n",
        "  'walk' should be keys of r\"\"\"\n",
        "  temp = 0\n",
        "  for step in reversed(walk):\n",
        "    temp = gamma* temp + r[step]\n",
        "    #print(temp)\n",
        "  return temp\n",
        "\n",
        "walk1 = ['C1', 'C2', 'C3', 'Pass', 'Sleep'];\n",
        "walk2 = ['C1', 'FB', 'FB', 'C1', 'C2', 'Sleep'];\n",
        "walk3 = ['C1', 'C2', 'C3', 'Pub', 'C2', 'C3', 'Pass', 'Sleep'];\n",
        "walk4 = ['FB', 'FB', 'FB', 'C1', 'C2', 'C3', 'Pub', 'C2', 'Sleep']\n",
        "\n",
        "print(\"Net reward in walk 1 is\", net_reward(walk1))\n",
        "print(\"Net reward in walk 2 is\", net_reward(walk2))\n",
        "print(\"Net reward in walk 3 is\", net_reward(walk3))\n",
        "print(\"Net reward in walk 4 is\", net_reward(walk4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net reward in walk 1 is -2.25\n",
            "Net reward in walk 2 is -3.125\n",
            "Net reward in walk 3 is -3.40625\n",
            "Net reward in walk 4 is -2.1875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT2Kokmmvrlm",
        "colab_type": "code",
        "outputId": "cd505d60-664c-4652-e976-75ce80e11d44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Value function with 'P' as the policy\n",
        "\n",
        "def value_fun(rewards = r, policy = P, iter = 100000, gamma = 0.9):\n",
        "  R = np.array(list(r.values()))\n",
        "  values = R\n",
        "  for i in range(iter):\n",
        "    #update rule using Bellman equation v = r + gamma*P*v\n",
        "    values = R + gamma*np.dot(P,values)\n",
        "  return values\n",
        "\n",
        "print(\"State-Value Function with gamma = 0 is \", np.round(value_fun(gamma =0.0),2))\n",
        "\n",
        "print(\"State-Value Function with gamma = 0.9 is \", np.round(value_fun(gamma =0.9),2))\n",
        "\n",
        "print(\"State-Value Function with gamma = 1 is \", np.round(value_fun(gamma =1),2))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State-Value Function with gamma = 0 is  [-2. -2. -2. 10.  1. -1.  0.]\n",
            "State-Value Function with gamma = 0.9 is  [-5.01  0.94  4.09 10.    1.91 -7.64  0.  ]\n",
            "State-Value Function with gamma = 1 is  [-12.54   1.46   4.32  10.     0.8  -22.54   0.  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIJ62fBVxujE",
        "colab_type": "text"
      },
      "source": [
        "# Part - two \n",
        "The resuts in this section requires the knowledge of Algorithms from Lecture 3\n",
        "\n",
        "State and Action Value Function for student Markov Decision Process\n",
        "1.   There are 5 states, denote them by -- S1,  S2, S3, S4, S5.\n",
        "2.   There are 6 actions in total -- Study, Facebook, Sleep,  Pub, Quit.\n",
        "3.   Not every action is available to the every state\n",
        "4.   Note that Reward depends on the state and action "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD0JcJ5Mx5fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Enivornment \n",
        "def MDP(s,a):\n",
        "  # given current state and action, output an array of triplets : \n",
        "  #(next_state, propability(next_state|state), reward(state,action)) \n",
        "  mdp={'S1' : {'Study' : [('S2',1,-2)], 'Facebook': [('S4',1,-1)]},\n",
        "       'S2' : {'Study' : [('S3',1,-2)], 'Sleep': [('S5',1,0)]},\n",
        "       'S3' : {'Study' : [('S5',1,10)], 'Pub' : [('S1',0.2,1), ('S2',0.4,1), ('S3',0.4,1)]},\n",
        "       'S4' : {'Quit' : [('S1',1,0)], 'Facebook': [('S4',1,-1)]},\n",
        "       'S5': {'Sleep' : [('S5',1,0)]}}\n",
        "  return mdp[s][a]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoN1ympQ-7rr",
        "colab_type": "text"
      },
      "source": [
        "Next we use the Value Iteration to compute the Value function and the Q function \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VByI5A7tDKA",
        "colab_type": "code",
        "outputId": "e5085571-ae6b-4ded-f580-dfe409163eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "\n",
        "#Consider the following policy\n",
        "policy = {'S1' : {'Study' : 0.5, 'Facebook': 0.5},\n",
        "       'S2' : {'Study' : 0.5, 'Sleep': 0.5},\n",
        "       'S3' : {'Study' : 0.5, 'Pub' : 0.5},\n",
        "       'S4' : {'Quit' : 0.5, 'Facebook': 0.5},\n",
        "       'S5': {'Sleep' : 1}}\n",
        "\n",
        "def Qvalue_function(policy = policy, iter_policy =500, gamma = 1):\n",
        "  # initializing Q-function and V-function as a function of state and action\n",
        "  V= {'S1':0,'S2':0,'S3':0,'S4':0,'S5':0}\n",
        "  Q= {'S1' : {'Study' : 0.5, 'Facebook': 0.5},\n",
        "       'S2' : {'Study' : 0.5, 'Sleep': 0.5},\n",
        "       'S3' : {'Study' : 0.5, 'Pub' : 0.5},\n",
        "       'S4' : {'Quit' : 0.5, 'Facebook': 0.5},\n",
        "       'S5': {'Sleep' : 1}}\n",
        "  # we will use both Q and V functions to find one another\n",
        "  #V(s) = Policy(s,a) * Q(s,a)\n",
        "  #Q(s,a) = R(s,a) + gamma* state_transitions(s,s') * V(s')\n",
        "  for i in range(iter_policy):\n",
        "    for s in ['S1','S2','S3','S4','S5']:\n",
        "      # introducing a dummy variable to hold the right side of V(s)\n",
        "      dummy = 0 \n",
        "      for a in policy[s]:\n",
        "        #we compute the second term in right hand side of equation Q(s,a) and store in the following variable\n",
        "        second_term= 0#{'S1':0,'S2':0,'S3':0,'S4':0,'S5':0}\n",
        "        #quiry the environment and collect the samples, i.e., next_state, P(state,next_state) and reward\n",
        "        for next_state, prob, reward in MDP(s,a):\n",
        "          second_term = second_term + prob * V[next_state]\n",
        "        Q[s][a] = reward + gamma * second_term\n",
        "        dummy = dummy + Q[s][a]*policy[s][a]\n",
        "      V[s] = dummy\n",
        "  return Q, V\n",
        "          \n",
        "Qvalue_function()\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'S1': {'Facebook': -3.3076923076923084, 'Study': 0.6923076923076925},\n",
              "  'S2': {'Sleep': 0, 'Study': 5.384615384615385},\n",
              "  'S3': {'Pub': 4.76923076923077, 'Study': 10},\n",
              "  'S4': {'Facebook': -3.3076923076923084, 'Quit': -1.307692307692308},\n",
              "  'S5': {'Sleep': 0}},\n",
              " {'S1': -1.307692307692308,\n",
              "  'S2': 2.6923076923076925,\n",
              "  'S3': 7.384615384615385,\n",
              "  'S4': -2.3076923076923084,\n",
              "  'S5': 0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s5QTcSovz-j",
        "colab_type": "text"
      },
      "source": [
        "Next, we find the optimal policy and (state) value functions using value iteration (with k=5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY8F0nUFodS8",
        "colab_type": "code",
        "outputId": "4bc678bb-6fd4-433a-aadd-6a65408f19d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "#Value Iteration\n",
        "policy = {'S1' : {'Study' : 0.5, 'Facebook': 0.5},\n",
        "       'S2' : {'Study' : 0.5, 'Sleep': 0.5},\n",
        "       'S3' : {'Study' : 0.5, 'Pub' : 0.5},\n",
        "       'S4' : {'Quit' : 0.5, 'Facebook': 0.5},\n",
        "       'S5': {'Sleep' : 1}}\n",
        "def Agent(policy = policy, iter_value = 100, iter_policy =5, gamma = 1):\n",
        "  #We now act greedily with respective to the Value function\n",
        "  for i in range(iter_value):\n",
        "    Q, V= Qvalue_function(policy = policy, iter_policy = iter_policy, gamma = gamma)\n",
        "    #We now act greedily with respective to Q\n",
        "    #One can also follow episilon greedily, which is actually a better way \n",
        "    for state in ['S1','S2','S3','S4','S5']:\n",
        "      opt_policy = max(Q[state], key=Q[state].get)\n",
        "      policy[state][opt_policy] = 1\n",
        "      not_opt_policy = min(Q[state], key=Q[state].get)\n",
        "      policy[state][not_opt_policy] = 0\n",
        "      #policy[state][max(Q[state], key=Q[state].get))] = 1\n",
        "      #policy[state][min(Q[state], key=Q[state].get))] = 0\n",
        "  return policy, Q, V\n",
        "\n",
        "\n",
        "optimal_policy, optimal_Q, optimal_V = Agent()\n",
        "\n",
        "print(\"Optimal Actions\")\n",
        "for state in ['S1','S2','S3','S4','S5']:\n",
        "  print(state, max(optimal_Q[state], key=optimal_Q[state].get))\n",
        "print(\"\\n\")\n",
        "print(\"Optimal State-Action Value function Q(s,a) is\")\n",
        "for state in ['S1','S2','S3','S4','S5']:\n",
        "  for action in  optimal_Q[state]:\n",
        "    print(state, action, optimal_Q[state][action])\n",
        "print(\"\\n\")\n",
        "print(\"Optimal State Value function V(s) is\")\n",
        "for state in ['S1','S2','S3','S4','S5']:\n",
        "  print(state, optimal_V[state])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal Actions\n",
            "S1 Study\n",
            "S2 Study\n",
            "S3 Study\n",
            "S4 Quit\n",
            "S5 Sleep\n",
            "\n",
            "\n",
            "Optimal State-Action Value function Q(s,a) is\n",
            "S1 Study 6.0\n",
            "S1 Facebook 5.0\n",
            "S2 Study 8.0\n",
            "S2 Sleep 0\n",
            "S3 Study 10\n",
            "S3 Pub 9.4\n",
            "S4 Quit 6.0\n",
            "S4 Facebook 5.0\n",
            "S5 Sleep 0\n",
            "\n",
            "\n",
            "Optimal State Value function V(s) is\n",
            "S1 6.0\n",
            "S2 8.0\n",
            "S3 10.0\n",
            "S4 6.0\n",
            "S5 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXRKUGh_raTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}